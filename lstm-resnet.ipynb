{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "66e36c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, Iterable, Sequence, Tuple, Optional, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "print(\"Using Tensorflow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4a75445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x4/ntfy4v_x0pd71m91lcwp9b940000gn/T/ipykernel_2158/2620712243.py:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  assert LooseVersion(tf.__version__) >= LooseVersion(\"2.0.0\"), \\\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion(\"2.0.0\"), \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d44b1e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_riskset(time: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Compute mask that represents each sample's risk set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    time : np.ndarray, shape=(n_samples,)\n",
    "        Observed event time sorted in descending order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    risk_set : np.ndarray, shape=(n_samples, n_samples)\n",
    "        Boolean matrix where the `i`-th row denotes the\n",
    "        risk set of the `i`-th instance, i.e. the indices `j`\n",
    "        for which the observer time `y_j >= y_i`.\n",
    "    \"\"\"\n",
    "    assert time.ndim == 1, \"expected 1D array\"\n",
    "\n",
    "    # sort in descending order\n",
    "    o = np.argsort(-time, kind=\"mergesort\")\n",
    "    n_samples = len(time)\n",
    "    risk_set = np.zeros((n_samples, n_samples), dtype=np.bool_)\n",
    "    for i_org, i_sort in enumerate(o):\n",
    "        ti = time[i_sort]\n",
    "        k = i_org\n",
    "        while k < n_samples and ti == time[o[k]]:\n",
    "            k += 1\n",
    "        risk_set[i_sort, o[:k]] = True\n",
    "    return risk_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0934a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFunction:\n",
    "    \"\"\"Callable input function that computes the risk set for each batch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    images : np.ndarray, shape=(n_samples, height, width)\n",
    "        Image data.\n",
    "    time : np.ndarray, shape=(n_samples,)\n",
    "        Observed time.\n",
    "    event : np.ndarray, shape=(n_samples,)\n",
    "        Event indicator.\n",
    "    batch_size : int, optional, default=64\n",
    "        Number of samples per batch.\n",
    "    drop_last : int, optional, default=False\n",
    "        Whether to drop the last incomplete batch.\n",
    "    shuffle : bool, optional, default=False\n",
    "        Whether to shuffle data.\n",
    "    seed : int, optional, default=89\n",
    "        Random number seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 images: np.ndarray,\n",
    "                 time: np.ndarray,\n",
    "                 event: np.ndarray,\n",
    "                 batch_size: int = 64,\n",
    "                 drop_last: bool = False,\n",
    "                 shuffle: bool = False,\n",
    "                 seed: int = 89) -> None:\n",
    "        if images.ndim == 3:\n",
    "            images = images[..., np.newaxis]\n",
    "        self.images = images\n",
    "        self.time = time\n",
    "        self.event = event\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Total number of samples.\"\"\"\n",
    "        return self.images.shape[0]\n",
    "\n",
    "    def steps_per_epoch(self) -> int:\n",
    "        \"\"\"Number of batches for one epoch.\"\"\"\n",
    "        return int(np.floor(self.size() / self.batch_size))\n",
    "\n",
    "    def _get_data_batch(self, index: np.ndarray) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
    "        \"\"\"Compute risk set for samples in batch.\"\"\"\n",
    "        time = self.time[index]\n",
    "        event = self.event[index]\n",
    "        images = self.images[index]\n",
    "\n",
    "        labels = {\n",
    "            \"label_event\": event.astype(np.int32),\n",
    "            \"label_time\": time.astype(np.float32),\n",
    "            \"label_riskset\": _make_riskset(time)\n",
    "        }\n",
    "        return images, labels\n",
    "\n",
    "    def _iter_data(self) -> Iterable[Tuple[np.ndarray, Dict[str, np.ndarray]]]:\n",
    "        \"\"\"Generator that yields one batch at a time.\"\"\"\n",
    "        index = np.arange(self.size())\n",
    "        rnd = np.random.RandomState(self.seed)\n",
    "\n",
    "        if self.shuffle:\n",
    "            rnd.shuffle(index)\n",
    "        for b in range(self.steps_per_epoch()):\n",
    "            start = b * self.batch_size\n",
    "            idx = index[start:(start + self.batch_size)]\n",
    "            yield self._get_data_batch(idx)\n",
    "\n",
    "        if not self.drop_last:\n",
    "            start = self.steps_per_epoch() * self.batch_size\n",
    "            idx = index[start:]\n",
    "            yield self._get_data_batch(idx)\n",
    "\n",
    "    def _get_shapes(self) -> Tuple[tf.TensorShape, Dict[str, tf.TensorShape]]:\n",
    "        \"\"\"Return shapes of data returned by `self._iter_data`.\"\"\"\n",
    "        batch_size = self.batch_size if self.drop_last else None\n",
    "        h, w, c = self.images.shape[1:]\n",
    "        images = tf.TensorShape([batch_size, h, w, c])\n",
    "\n",
    "        labels = {k: tf.TensorShape((batch_size,))\n",
    "                  for k in (\"label_event\", \"label_time\")}\n",
    "        labels[\"label_riskset\"] = tf.TensorShape((batch_size, batch_size))\n",
    "        return images, labels\n",
    "\n",
    "    def _get_dtypes(self) -> Tuple[tf.DType, Dict[str, tf.DType]]:\n",
    "        \"\"\"Return dtypes of data returned by `self._iter_data`.\"\"\"\n",
    "        labels = {\"label_event\": tf.int32,\n",
    "                  \"label_time\": tf.float32,\n",
    "                  \"label_riskset\": tf.bool}\n",
    "        return tf.float32, labels\n",
    "\n",
    "    def _make_dataset(self) -> tf.data.Dataset:\n",
    "        \"\"\"Create dataset from generator.\"\"\"\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            self._iter_data,\n",
    "            self._get_dtypes(),\n",
    "            self._get_shapes()\n",
    "        )\n",
    "        return ds\n",
    "\n",
    "    def __call__(self) -> tf.data.Dataset:\n",
    "        return self._make_dataset()\n",
    "\n",
    "\n",
    "def safe_normalize(x: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Normalize risk scores to avoid exp underflowing.\n",
    "\n",
    "    Note that only risk scores relative to each other matter.\n",
    "    If minimum risk score is negative, we shift scores so minimum\n",
    "    is at zero.\n",
    "    \"\"\"\n",
    "    x_min = tf.reduce_min(x, axis=0)\n",
    "    c = tf.zeros_like(x_min)\n",
    "    norm = tf.where(x_min < 0, -x_min, c)\n",
    "    return x + norm\n",
    "\n",
    "\n",
    "def logsumexp_masked(risk_scores: tf.Tensor,\n",
    "                     mask: tf.Tensor,\n",
    "                     axis: int = 0,\n",
    "                     keepdims: Optional[bool] = None) -> tf.Tensor:\n",
    "    \"\"\"Compute logsumexp across `axis` for entries where `mask` is true.\"\"\"\n",
    "    risk_scores.shape.assert_same_rank(mask.shape)\n",
    "\n",
    "    with tf.name_scope(\"logsumexp_masked\"):\n",
    "        mask_f = tf.cast(mask, risk_scores.dtype)\n",
    "        risk_scores_masked = tf.math.multiply(risk_scores, mask_f)\n",
    "        # for numerical stability, substract the maximum value\n",
    "        # before taking the exponential\n",
    "        amax = tf.reduce_max(risk_scores_masked, axis=axis, keepdims=True)\n",
    "        risk_scores_shift = risk_scores_masked - amax\n",
    "\n",
    "        exp_masked = tf.math.multiply(tf.exp(risk_scores_shift), mask_f)\n",
    "        exp_sum = tf.reduce_sum(exp_masked, axis=axis, keepdims=True)\n",
    "        output = amax + tf.math.log(exp_sum)\n",
    "        if not keepdims:\n",
    "            output = tf.squeeze(output, axis=axis)\n",
    "    return output\n",
    "\n",
    "\n",
    "class CoxPHLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Negative partial log-likelihood of Cox's proportional hazards model.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)            \n",
    "\n",
    "    def call(self,\n",
    "             y_true: Sequence[tf.Tensor],\n",
    "             y_pred: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Compute loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : list|tuple of tf.Tensor\n",
    "            The first element holds a binary vector where 1\n",
    "            indicates an event 0 censoring.\n",
    "            The second element holds the riskset, a\n",
    "            boolean matrix where the `i`-th row denotes the\n",
    "            risk set of the `i`-th instance, i.e. the indices `j`\n",
    "            for which the observer time `y_j >= y_i`.\n",
    "            Both must be rank 2 tensors.\n",
    "        y_pred : tf.Tensor\n",
    "            The predicted outputs. Must be a rank 2 tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : tf.Tensor\n",
    "            Loss for each instance in the batch.\n",
    "        \"\"\"\n",
    "        event, riskset = y_true\n",
    "        predictions = y_pred\n",
    "\n",
    "        pred_shape = predictions.shape\n",
    "        if pred_shape.ndims != 2:\n",
    "            raise ValueError(\"Rank mismatch: Rank of predictions (received %s) should \"\n",
    "                             \"be 2.\" % pred_shape.ndims)\n",
    "\n",
    "        if pred_shape[1] is None:\n",
    "            raise ValueError(\"Last dimension of predictions must be known.\")\n",
    "\n",
    "        if pred_shape[1] != 1:\n",
    "            raise ValueError(\"Dimension mismatch: Last dimension of predictions \"\n",
    "                             \"(received %s) must be 1.\" % pred_shape[1])\n",
    "\n",
    "        if event.shape.ndims != pred_shape.ndims:\n",
    "            raise ValueError(\"Rank mismatch: Rank of predictions (received %s) should \"\n",
    "                             \"equal rank of event (received %s)\" % (\n",
    "                pred_shape.ndims, event.shape.ndims))\n",
    "\n",
    "        if riskset.shape.ndims != 2:\n",
    "            raise ValueError(\"Rank mismatch: Rank of riskset (received %s) should \"\n",
    "                             \"be 2.\" % riskset.shape.ndims)\n",
    "\n",
    "        event = tf.cast(event, predictions.dtype)\n",
    "        predictions = safe_normalize(predictions)\n",
    "        with tf.name_scope(\"assertions\"):\n",
    "            assertions = (\n",
    "                tf.debugging.assert_less_equal(event, 1.),\n",
    "                tf.debugging.assert_greater_equal(event, 0.),\n",
    "                tf.debugging.assert_type(riskset, tf.bool)\n",
    "            )\n",
    "\n",
    "        # move batch dimension to the end so predictions get broadcast\n",
    "        # row-wise when multiplying by riskset\n",
    "        pred_t = tf.transpose(predictions)\n",
    "        # compute log of sum over risk set for each row\n",
    "        rr = logsumexp_masked(pred_t, riskset, axis=1, keepdims=True)\n",
    "        assert rr.shape.as_list() == predictions.shape.as_list()\n",
    "\n",
    "        losses = tf.math.multiply(event, rr - predictions)\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cec095ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CindexMetric:\n",
    "    \"\"\"Computes concordance index across one epoch.\"\"\"\n",
    "\n",
    "    def reset_states(self) -> None:\n",
    "        \"\"\"Clear the buffer of collected values.\"\"\"\n",
    "        self._data = {\n",
    "            \"label_time\": [],\n",
    "            \"label_event\": [],\n",
    "            \"prediction\": []\n",
    "        }\n",
    "\n",
    "    def update_state(self, y_true: Dict[str, tf.Tensor], y_pred: tf.Tensor) -> None:\n",
    "        \"\"\"Collect observed time, event indicator and predictions for a batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : dict\n",
    "            Must have two items:\n",
    "            `label_time`, a tensor containing observed time for one batch,\n",
    "            and `label_event`, a tensor containing event indicator for one batch.\n",
    "        y_pred : tf.Tensor\n",
    "            Tensor containing predicted risk score for one batch.\n",
    "        \"\"\"\n",
    "        self._data[\"label_time\"].append(y_true[\"label_time\"].numpy())\n",
    "        self._data[\"label_event\"].append(y_true[\"label_event\"].numpy())\n",
    "        self._data[\"prediction\"].append(tf.squeeze(y_pred).numpy())\n",
    "\n",
    "    def result(self) -> Dict[str, float]:\n",
    "        \"\"\"Computes the concordance index across collected values.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        metrics : dict\n",
    "            Computed metrics.\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        for k, v in self._data.items():\n",
    "            data[k] = np.concatenate(v)\n",
    "\n",
    "        results = concordance_index_censored(\n",
    "            data[\"label_event\"] == 1,\n",
    "            data[\"label_time\"],\n",
    "            data[\"prediction\"])\n",
    "\n",
    "        result_data = {}\n",
    "        names = (\"cindex\", \"concordant\", \"discordant\", \"tied_risk\")\n",
    "        for k, v in zip(names, results):\n",
    "            result_data[k] = v\n",
    "        return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c5f9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2.summary as summary\n",
    "from tensorflow.python.ops import summary_ops_v2\n",
    "\n",
    "\n",
    "class TrainAndEvaluateModel:\n",
    "\n",
    "    def __init__(self, model, model_dir, train_dataset, eval_dataset,\n",
    "                 learning_rate, num_epochs):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.model_dir = model_dir\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.train_ds = train_dataset\n",
    "        self.val_ds = eval_dataset\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.loss_fn = CoxPHLoss()\n",
    "\n",
    "        self.train_loss_metric = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "        self.val_loss_metric = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "        self.val_cindex_metric = CindexMetric()\n",
    "\n",
    "    @tf.function\n",
    "    def train_one_step(self, x, y_event, y_riskset):\n",
    "        y_event = tf.expand_dims(y_event, axis=1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(x, training=True)\n",
    "\n",
    "            train_loss = self.loss_fn(y_true=[y_event, y_riskset], y_pred=logits)\n",
    "\n",
    "        with tf.name_scope(\"gradients\"):\n",
    "            grads = tape.gradient(train_loss, self.model.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        return train_loss, logits\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            step=tf.Variable(0, dtype=tf.int64),\n",
    "            optimizer=self.optimizer,\n",
    "            model=self.model)\n",
    "        ckpt_manager = tf.train.CheckpointManager(\n",
    "            ckpt, str(self.model_dir), max_to_keep=2)\n",
    "\n",
    "        if ckpt_manager.latest_checkpoint:\n",
    "            ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Latest checkpoint restored from {ckpt_manager.latest_checkpoint}.\")\n",
    "\n",
    "        train_summary_writer = summary.create_file_writer(\n",
    "            str(self.model_dir / \"train\"))\n",
    "        val_summary_writer = summary.create_file_writer(\n",
    "            str(self.model_dir / \"valid\"))\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            with train_summary_writer.as_default():\n",
    "                self.train_one_epoch(ckpt.step)\n",
    "\n",
    "            # Run a validation loop at the end of each epoch.\n",
    "            with val_summary_writer.as_default():\n",
    "                self.evaluate(ckpt.step)\n",
    "\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(f\"Saved checkpoint for step {ckpt.step.numpy()}: {save_path}\")\n",
    "    def train_one_epoch(self, step_counter):\n",
    "        for x, y in self.train_ds:\n",
    "            train_loss, logits = self.train_one_step(\n",
    "                x, y[\"label_event\"], y[\"label_riskset\"])\n",
    "\n",
    "            step = int(step_counter)\n",
    "            if step == 0:\n",
    "                # see https://stackoverflow.com/questions/58843269/display-graph-using-tensorflow-v2-0-in-tensorboard\n",
    "                func = self.train_one_step.get_concrete_function(\n",
    "                    x, y[\"label_event\"], y[\"label_riskset\"])\n",
    "                summary_ops_v2.graph(func.graph)\n",
    "\n",
    "            # Update training metric.\n",
    "            self.train_loss_metric.update_state(train_loss)\n",
    "\n",
    "            # Log every 200 batches.\n",
    "            if step % 200 == 0:\n",
    "                # Display metrics\n",
    "                mean_loss = self.train_loss_metric.result()\n",
    "                print(f\"step {step}: mean loss = {mean_loss:.4f}\")\n",
    "                # save summaries\n",
    "                summary.scalar(\"loss\", mean_loss, step=step_counter)\n",
    "                # Reset training metrics\n",
    "                self.train_loss_metric.reset_states()\n",
    "\n",
    "            step_counter.assign_add(1)\n",
    "\n",
    "    @tf.function\n",
    "    def evaluate_one_step(self, x, y_event, y_riskset):\n",
    "        y_event = tf.expand_dims(y_event, axis=1)\n",
    "        val_logits = self.model(x, training=False)\n",
    "        val_loss = self.loss_fn(y_true=[y_event, y_riskset], y_pred=val_logits)\n",
    "        return val_loss, val_logits\n",
    "\n",
    "    def evaluate(self, step_counter):\n",
    "        self.val_cindex_metric.reset_states()\n",
    "        \n",
    "        for x_val, y_val in self.val_ds:\n",
    "            val_loss, val_logits = self.evaluate_one_step(\n",
    "                x_val, y_val[\"label_event\"], y_val[\"label_riskset\"])\n",
    "\n",
    "            # Update val metrics\n",
    "            self.val_loss_metric.update_state(val_loss)\n",
    "            self.val_cindex_metric.update_state(y_val, val_logits)\n",
    "\n",
    "        val_loss = self.val_loss_metric.result()\n",
    "        summary.scalar(\"loss\",\n",
    "                       val_loss,\n",
    "                       step=step_counter)\n",
    "        self.val_loss_metric.reset_states()\n",
    "        \n",
    "        val_cindex = self.val_cindex_metric.result()\n",
    "        for key, value in val_cindex.items():\n",
    "            summary.scalar(key, value, step=step_counter)\n",
    "\n",
    "        print(f\"Validation: loss = {val_loss:.4f}, cindex = {val_cindex['cindex']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d5e55911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "71988aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `Sequential` from `keras.models`\n",
    "from keras.models import Sequential\n",
    "# Import `Dense` from `keras.layers`\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "import keras\n",
    "#main_input = Input(shape=(data_new.shape[1], data_new.shape[2]), name='main_input')\n",
    "model = tf.keras.Sequential([\n",
    "    #old\n",
    "    #tf.keras.layers.LSTM(64,input_shape=(4,52)),\n",
    "    #tf.keras.layers.Dense(64, activation='relu'),\n",
    "    #tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    #tf.keras.layers.Dense(1, activation='relu')\n",
    "    #tf.keras.layers.Dense(1, activation='tanh')\n",
    "    #tf.keras.layers.Dense(1, activation='linear')\n",
    "   ##new\n",
    "    tf.keras.layers.LSTM(64, input_shape=(5,45), return_sequences=True),\n",
    "   # tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.LSTM(16, return_sequences=True),  \n",
    "    tf.keras.layers.LSTM(8),\n",
    "    #tf.keras.layers.Dense(1, activation='linear')\n",
    "    #tf.keras.layers.Dense(1, activation='tanh')\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    #tf.keras.layers.Dense(1, activation='relu')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b2be612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_fn = InputFunction(data_new, np.array( y_new[:,0]), np.array( y_new[:,1]),\n",
    "                  drop_last=True,\n",
    "                  shuffle=True)\n",
    "\n",
    "eval_fn = InputFunction(data_new_test, np.array(y_new_test[:,0]), np.array( y_new_test[:,1]))\n",
    "\n",
    "trainer = TrainAndEvaluateModel(\n",
    "    model=model,\n",
    "    model_dir=Path(\"ckpts\"),\n",
    "    train_dataset=train_fn(),\n",
    "    eval_dataset=eval_fn(),\n",
    "    learning_rate=0.00001,\n",
    "    num_epochs=45,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44de3402",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path='data/temporal'\n",
    "#X = np.load(path.join(data_path,'X.csv'))\n",
    "X=pd.read_csv(\"data/temporal/X5_all_arti.csv\")\n",
    "\n",
    "X=X.iloc[:,1:]\n",
    "#print(X)\n",
    "#y = np.load(path.join(data_path,'y.npy'))\n",
    "y=pd.read_csv(\"data/temporal/Y5_e_arti.csv\")\n",
    "#print(y.iloc[:,1:])\n",
    "y=y.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8650c2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "vor_images1=np.load(\"locf5_image (2).npy\")\n",
    "#vor_images1=vor_images1.iloc[:,1:]\n",
    "#vor_images2=vor_images1.values.reshape(16728, 1,8,9)\n",
    "#print(vor_images1.shape)\n",
    "#print(vor_images1)\n",
    "vor_images1=torch.from_numpy(vor_images1)\n",
    "#vor_images1=vor_images1.to(torch.float32)\n",
    "vor_images1=pd.DataFrame(vor_images1)\n",
    "##print(vor_images2.shape)\n",
    "#print(vor_images1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37632be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "            from sklearn.model_selection import  StratifiedKFold \n",
    "            rskf =  StratifiedKFold (n_splits=10, shuffle=True,random_state=412)\n",
    "            index = 1\n",
    "            features_per_folds = []\n",
    "            for train_index, test_index in rskf.split(vor_images1,y.iloc[:,0]):\n",
    "               # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                #print(\"iter\", index)\n",
    "                index += 1\n",
    "                X_train, X_test = vor_images1.iloc[train_index,:], vor_images1.iloc[test_index,:]\n",
    "                y_train, y_test = X.iloc[train_index,54:56], X.iloc[test_index,54:56]\n",
    "                #print(X_train.shape[0])\n",
    "                #print(X_test.shape[0])\n",
    "                t=5\n",
    "                dim1=(int(X_train.shape[0]/t))\n",
    "                dim2=(int(X_test.shape[0]/t))\n",
    "                data_new=np.zeros((dim1,5,45))\n",
    "                data_new_test=np.zeros((dim2,5,45))\n",
    "                y_new=np.zeros((dim1,2))\n",
    "                y_new_test=np.zeros((dim2,2))\n",
    "#X_train0=X_train.values.tolist()\n",
    "                X_train0=X_train.values\n",
    "                X_train_lists = [X_train0[i:i + t] for i in range(0, len(X_train0), t)]\n",
    "                X_train_lists_array=np.asarray(X_train_lists)\n",
    "\n",
    "                y_train0=y_train.values\n",
    "                y_train_lists = [y_train0[i:i + t] for i in range(0, len(y_train0), t)]\n",
    "                y_train_lists_array=np.asarray(y_train_lists)\n",
    "\n",
    "\n",
    "\n",
    "                X_test0=X_test.values\n",
    "                X_test_lists = [X_test0[i:i + t] for i in range(0, len(X_test0), t)]\n",
    "                X_test_lists_array=np.asarray(X_test_lists)\n",
    "\n",
    "                y_test0=y_test.values\n",
    "                y_test_lists = [y_test0[i:i + t] for i in range(0, len(y_test0), t)]\n",
    "                y_test_lists_array=np.asarray(y_test_lists)\n",
    "                for i in range(int(X_train.shape[0]/t)):\n",
    "                    data_new[i]=X_train_lists_array[i]\n",
    "   \n",
    "                    y_new[i]=y_train_lists_array[i][1]\n",
    "    #return(data_new)\n",
    "#data_new.shape\n",
    "                for i in range(int(X_test.shape[0]/t)):\n",
    "                   data_new_test[i]=X_test_lists_array[i]\n",
    "                   y_new_test[i]=y_test_lists_array[i][1]\n",
    "    #return(data_new)\n",
    "                trainer.train_and_evaluate()\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
