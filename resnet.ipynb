{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0006967",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import sys \n",
    "from ast import literal_eval\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "#from scipy.misc import imresize\n",
    "from keras.callbacks import EarlyStopping, Callback, LearningRateScheduler, ModelCheckpoint\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D, GlobalAveragePooling2D, MaxPooling2D, UpSampling2D, Concatenate, Reshape\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.noise import GaussianNoise\n",
    "#from keras.optimizers import SGD, Adam\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
    "from keras import backend\n",
    "from keras.models import load_model \n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve, precision_recall_fscore_support, f1_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "\n",
    "class Data_Fold:\n",
    "    \"\"\"\n",
    "    Data class with attributes below. \n",
    "    This puts in a structured way the raw data and images for one fold.\n",
    "    \"\"\"\n",
    "    tr_input_imgs   = []\n",
    "    val_input_imgs  = []\n",
    "    test_input_imgs = []\n",
    "    tr_input_vf     = []\n",
    "    val_input_vf    = []\n",
    "    test_input_vf   = []\n",
    "   # test_md  = []\n",
    "   # test_slv = []\n",
    "    tr_output   = []\n",
    "    val_output  = []\n",
    "    test_output = []\n",
    "    train_pat_idents = []\n",
    "    val_pat_idents   = []\n",
    "    test_pat_idents  = []\n",
    "    train_idx        = []\n",
    "    val_idx          = []\n",
    "    test_idx         = []\n",
    "def save_object(obj, filename):\n",
    "    \"\"\"\n",
    "    Saves an object ot a file\n",
    "    Inputs:\n",
    "        obj: Object to write into the file\n",
    "        filename: The filename to write into\n",
    "    Outputs:\n",
    "        File with filename\n",
    "    \"\"\"        \n",
    "    with open(filename, 'wb') as output:  # Overwrites any existing file.\n",
    "        pickle.dump(obj, output, pickle.HIGHEST_PROTOCOL)\n",
    "def load_object(filename):\n",
    "    \"\"\"\n",
    "    Load the object in a pickle file into the environment\n",
    "    Inputs: \n",
    "        filename: the filename were object to load was saved\n",
    "    Outputs:\n",
    "        obj: the object of interest, can be a list of objects\n",
    "    \"\"\"\n",
    "    with open(filename, 'rb') as input:\n",
    "        obj = pickle.load(input)\n",
    "\n",
    "    return obj\n",
    "\n",
    "def create_data(df):\n",
    "    \"\"\"\n",
    "    Creates a data object including all data folds used in training, validation \n",
    "    and test.\n",
    "    Inputs:\n",
    "        dataset: Dataset name. 'BD' for Budapest dataset, 'RT' for Rotterdam dataset\n",
    "    Outputs:\n",
    "        data_folds: Data fold object (see Data_Fold class)\n",
    "    \"\"\"\n",
    "\n",
    "#     if dataset == 'BD':\n",
    "#         num_locs = 38\n",
    "#     elif dataset == 'RT':\n",
    "       # num_locs == 52\n",
    "#     else:\n",
    "#         print('No dataset is available with that name!')\n",
    "    \n",
    "#     df = pd.read_csv('./S1_data/{}_dataset.csv'.format(dataset))\n",
    "    patient_id = np.asarray(df['PATIENT.ID'])\n",
    "    eye_id     = np.asarray(df['EYE.ID'])[:,np.newaxis]\n",
    "   # md         = np.asarray(df['MD'])[:,np.newaxis]\n",
    "   # slv        = np.asarray(df['sLV'])[:,np.newaxis]\n",
    "    labels     = np.asarray(df['GLAUCOMATOUS'])[:,np.newaxis]\n",
    "    vf_data = np.zeros((len(labels), num_locs))\n",
    "    x = np.zeros((len(labels), num_locs))\n",
    "    y = np.zeros((len(labels), num_locs))\n",
    "    for k in range(num_locs):\n",
    "        vf_data[:, k] = np.asarray(df['VF_{:d}'.format(k+1)])\n",
    "        x[:,k] = np.asarray(df['X_{:d}'.format(k+1)])\n",
    "        y[:,k] = np.asarray(df['Y_{:d}'.format(k+1)])\n",
    "\n",
    "    # Append global indices to VF data matrix\n",
    "    # vf_data = np.concatenate([vf_data], axis=1)\n",
    "   # vf_data = np.concatenate([vf_data, slv], axis=1)\n",
    "\n",
    "    # Map all coordinates to right eye format\n",
    "    x[eye_id[:,0]==1, :] = -1 * x[eye_id[:,0]==1, :]\n",
    "\n",
    "    # Create voronoi images\n",
    "    xy = np.concatenate([x[0, np.newaxis].T, y[0, np.newaxis].T], axis=1)\n",
    "    vor_images = generate_voronoi_images_given_image_size(vf_data, xy)\n",
    "\n",
    "    # Read training, validation and test splits\n",
    "    tr_pat_ids = []\n",
    "    val_pat_ids = []\n",
    "    test_pat_ids = []\n",
    "    with open('./S1_data/{}_training_patient_ids.csv'.format(dataset), 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            tr_pat_ids.append(np.asarray(row).astype('int'))\n",
    "\n",
    "    with open('./S1_data/{}_validation_patient_ids.csv'.format(dataset), 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            val_pat_ids.append(np.asarray(row).astype('int'))\n",
    "\n",
    "    with open('./S1_data/{}_test_patient_ids.csv'.format(dataset), 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in reader:\n",
    "            test_pat_ids.append(np.asarray(row).astype('int'))\n",
    "\n",
    "    # Create data object with data splits\n",
    "    data_folds = []\n",
    "    num_folds = 10\n",
    "    for f in range(num_folds):\n",
    "\n",
    "        train_idx = np.isin(patient_id, tr_pat_ids[f])\n",
    "        val_idx = np.isin(patient_id, val_pat_ids[f])\n",
    "        test_idx = np.isin(patient_id, test_pat_ids[f])\n",
    "\n",
    "        data_fold = Data_Fold()\n",
    "\n",
    "        # Voronoi images\n",
    "        data_fold.tr_input_imgs   = vor_images[train_idx]\n",
    "        data_fold.val_input_imgs  = vor_images[val_idx]\n",
    "        data_fold.test_input_imgs = vor_images[test_idx]\n",
    "        \n",
    "        # Linear VF vectors (total deviation)\n",
    "        data_fold.tr_input_vf      = vf_data[train_idx]\n",
    "        data_fold.val_input_vf     = vf_data[val_idx]\n",
    "        data_fold.test_input_vf    = vf_data[test_idx]\n",
    "        \n",
    "        # MD, SLV\n",
    "        #data_fold.train_md = md[train_idx]\n",
    "        #data_fold.val_md   = md[val_idx]\n",
    "        #data_fold.test_md  = md[test_idx]\n",
    "       # data_fold.train_slv = slv[train_idx]\n",
    "      #  data_fold.val_slv   = slv[val_idx]\n",
    "       # data_fold.test_slv  = slv[test_idx]\n",
    "\n",
    "        # Coordinates\n",
    "        data_fold.xy       = xy\n",
    "\n",
    "        # Labels\n",
    "        data_fold.tr_output   = labels[train_idx]\n",
    "        data_fold.val_output  = labels[val_idx]\n",
    "        data_fold.test_output = labels[test_idx]\n",
    "        \n",
    "        # Patient and samples indices\n",
    "        data_fold.train_pat_idents = patient_id[train_idx]\n",
    "        data_fold.val_pat_idents   = patient_id[val_idx]\n",
    "        data_fold.test_pat_idents  = patient_id[test_idx]\n",
    "        data_fold.train_idx        = train_idx.copy()\n",
    "        data_fold.val_idx          = val_idx.copy()\n",
    "        data_fold.test_idx         = test_idx.copy()\n",
    "        \n",
    "        data_folds.append(data_fold)\n",
    "\n",
    "    return data_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bb886a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86745877",
   "metadata": {},
   "outputs": [],
   "source": [
    "    patient_id = np.asarray(df['PATIENT.ID'])\n",
    "    eye_id     = np.asarray(df['EYE.ID'])[:,np.newaxis]\n",
    "   # md         = np.asarray(df['MD'])[:,np.newaxis]\n",
    "   # slv        = np.asarray(df['sLV'])[:,np.newaxis]\n",
    "   # labels     = np.asarray(df['GLAUCOMATOUS'])[:,np.newaxis]\n",
    "    vf_data = np.zeros((len(eye_id), num_locs))\n",
    "    x = np.zeros((len(eye_id), num_locs))\n",
    "    y = np.zeros((len(eye_id), num_locs))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6a99ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14130, 1, 8, 9)\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "vor_images1=pd.read_csv(\"data/temporal/data_72_arti.csv\",header=None)\n",
    "#vor_images1=vor_images1\n",
    "#vor_images1\n",
    "vor_images2=vor_images1.values.reshape(14130, 1,8,9)\n",
    "print(vor_images2.shape)\n",
    "#print(vor_images1)\n",
    "vor_images2=torch.from_numpy(vor_images2)\n",
    "vor_images2=vor_images2.to(torch.float32)\n",
    "#print(vor_images2.shape)\n",
    "print(vor_images2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae1c1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class block(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_channels, intermediate_channels, identity_downsample=None, stride=1\n",
    "    ):\n",
    "        super(block, self).__init__()\n",
    "        self.expansion = 4\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            intermediate_channels,\n",
    "            intermediate_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            intermediate_channels,\n",
    "            intermediate_channels * self.expansion,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=False\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(intermediate_channels * self.expansion)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.identity_downsample = identity_downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x.clone()\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        if self.identity_downsample is not None:\n",
    "            identity = self.identity_downsample(identity)\n",
    "\n",
    "        x += identity\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, image_channels, num_classes):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(image_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Essentially the entire ResNet architecture are in these 4 lines below\n",
    "        self.layer1 = self._make_layer(\n",
    "            block, layers[0], intermediate_channels=64, stride=1\n",
    "        )\n",
    "        self.layer2 = self._make_layer(\n",
    "            block, layers[1], intermediate_channels=128, stride=2\n",
    "        )\n",
    "        self.layer3 = self._make_layer(\n",
    "            block, layers[2], intermediate_channels=256, stride=2\n",
    "        )\n",
    "        self.layer4 = self._make_layer(\n",
    "            block, layers[3], intermediate_channels=512, stride=2\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, num_residual_blocks, intermediate_channels, stride):\n",
    "        identity_downsample = None\n",
    "        layers = []\n",
    "\n",
    "        # Either if we half the input space for ex, 56x56 -> 28x28 (stride=2), or channels changes\n",
    "        # we need to adapt the Identity (skip connection) so it will be able to be added\n",
    "        # to the layer that's ahead\n",
    "        if stride != 1 or self.in_channels != intermediate_channels * 4:\n",
    "            identity_downsample = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    self.in_channels,\n",
    "                    intermediate_channels * 4,\n",
    "                    kernel_size=1,\n",
    "                    stride=stride,\n",
    "                    bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(intermediate_channels * 4),\n",
    "            )\n",
    "\n",
    "        layers.append(\n",
    "            block(self.in_channels, intermediate_channels, identity_downsample, stride)\n",
    "        )\n",
    "\n",
    "        # The expansion size is always 4 for ResNet 50,101,152\n",
    "        self.in_channels = intermediate_channels * 4\n",
    "\n",
    "        # For example for first resnet layer: 256 will be mapped to 64 as intermediate layer,\n",
    "        # then finally back to 256. Hence no identity downsample is needed, since stride = 1,\n",
    "        # and also same amount of channels.\n",
    "        for i in range(num_residual_blocks - 1):\n",
    "            layers.append(block(self.in_channels, intermediate_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def ResNet50(img_channel=3, num_classes=1000):\n",
    "    return ResNet(block, [3, 4, 6, 3], img_channel, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#def test():\n",
    "#    net = ResNet50(img_channel=1, num_classes=52)\n",
    "#    y1 = net(vor_images1[0:2000,:])\n",
    "#    y2=y1.detach().numpy(  )\n",
    "#    np.save('locf6_2000',y2)\n",
    "#    print(y2.shape)\n",
    "#    print(y1.shape)\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b2bcc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "    net = ResNet50(img_channel=1, num_classes=45)\n",
    "    y3 = net(vor_images2)\n",
    "    y4=y3.detach().numpy(  )\n",
    "    np.save(locf5_image (2).npy, y4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
