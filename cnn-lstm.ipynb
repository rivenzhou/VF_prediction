{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66e36c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Tensorflow: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, Iterable, Sequence, Tuple, Optional, Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sksurv.nonparametric import kaplan_meier_estimator\n",
    "from sksurv.metrics import concordance_index_censored\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "print(\"Using Tensorflow:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a75445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZHOUR\\AppData\\Local\\Temp\\ipykernel_38852\\2620712243.py:3: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  assert LooseVersion(tf.__version__) >= LooseVersion(\"2.0.0\"), \\\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion(\"2.0.0\"), \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0934a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFunction:\n",
    "    \"\"\"Callable input function that computes the risk set for each batch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    images : np.ndarray, shape=(n_samples, height, width)\n",
    "        Image data.\n",
    "    time : np.ndarray, shape=(n_samples,)\n",
    "        Observed time.\n",
    "    event : np.ndarray, shape=(n_samples,)\n",
    "        Event indicator.\n",
    "    batch_size : int, optional, default=64\n",
    "        Number of samples per batch.\n",
    "    drop_last : int, optional, default=False\n",
    "        Whether to drop the last incomplete batch.\n",
    "    shuffle : bool, optional, default=False\n",
    "        Whether to shuffle data.\n",
    "    seed : int, optional, default=89\n",
    "        Random number seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 images: np.ndarray,\n",
    "                 time: np.ndarray,\n",
    "                 event: np.ndarray,\n",
    "                 batch_size: int = 64,\n",
    "                 drop_last: bool = False,\n",
    "                 shuffle: bool = False,\n",
    "                 seed: int = 89) -> None:\n",
    "        if images.ndim == 3:\n",
    "            images = images[..., np.newaxis]\n",
    "        self.images = images\n",
    "        self.time = time\n",
    "        self.event = event\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "\n",
    "    def size(self) -> int:\n",
    "        \"\"\"Total number of samples.\"\"\"\n",
    "        return self.images.shape[0]\n",
    "\n",
    "    def steps_per_epoch(self) -> int:\n",
    "        \"\"\"Number of batches for one epoch.\"\"\"\n",
    "        return int(np.floor(self.size() / self.batch_size))\n",
    "\n",
    "    def _get_data_batch(self, index: np.ndarray) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:\n",
    "        \"\"\"Compute risk set for samples in batch.\"\"\"\n",
    "        time = self.time[index]\n",
    "        event = self.event[index]\n",
    "        images = self.images[index]\n",
    "\n",
    "        labels = {\n",
    "            \"label_event\": event.astype(np.int32),\n",
    "            \"label_time\": time.astype(np.float32),\n",
    "            \"label_riskset\": _make_riskset(time)\n",
    "        }\n",
    "        return images, labels\n",
    "\n",
    "    def _iter_data(self) -> Iterable[Tuple[np.ndarray, Dict[str, np.ndarray]]]:\n",
    "        \"\"\"Generator that yields one batch at a time.\"\"\"\n",
    "        index = np.arange(self.size())\n",
    "        rnd = np.random.RandomState(self.seed)\n",
    "\n",
    "        if self.shuffle:\n",
    "            rnd.shuffle(index)\n",
    "        for b in range(self.steps_per_epoch()):\n",
    "            start = b * self.batch_size\n",
    "            idx = index[start:(start + self.batch_size)]\n",
    "            yield self._get_data_batch(idx)\n",
    "\n",
    "        if not self.drop_last:\n",
    "            start = self.steps_per_epoch() * self.batch_size\n",
    "            idx = index[start:]\n",
    "            yield self._get_data_batch(idx)\n",
    "\n",
    "    def _get_shapes(self) -> Tuple[tf.TensorShape, Dict[str, tf.TensorShape]]:\n",
    "        \"\"\"Return shapes of data returned by `self._iter_data`.\"\"\"\n",
    "        batch_size = self.batch_size if self.drop_last else None\n",
    "        l, h, w, c = self.images.shape[1:]\n",
    "        images = tf.TensorShape([batch_size, l,h, w, c])\n",
    "\n",
    "        labels = {k: tf.TensorShape((batch_size,))\n",
    "                  for k in (\"label_event\", \"label_time\")}\n",
    "        labels[\"label_riskset\"] = tf.TensorShape((batch_size, batch_size))\n",
    "        return images, labels\n",
    "\n",
    "    def _get_dtypes(self) -> Tuple[tf.DType, Dict[str, tf.DType]]:\n",
    "        \"\"\"Return dtypes of data returned by `self._iter_data`.\"\"\"\n",
    "        labels = {\"label_event\": tf.int32,\n",
    "                  \"label_time\": tf.float32,\n",
    "                  \"label_riskset\": tf.bool}\n",
    "        return tf.float32, labels\n",
    "\n",
    "    def _make_dataset(self) -> tf.data.Dataset:\n",
    "        \"\"\"Create dataset from generator.\"\"\"\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            self._iter_data,\n",
    "            self._get_dtypes(),\n",
    "            self._get_shapes()\n",
    "        )\n",
    "        return ds\n",
    "\n",
    "    def __call__(self) -> tf.data.Dataset:\n",
    "        return self._make_dataset()\n",
    "\n",
    "\n",
    "def safe_normalize(x: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Normalize risk scores to avoid exp underflowing.\n",
    "\n",
    "    Note that only risk scores relative to each other matter.\n",
    "    If minimum risk score is negative, we shift scores so minimum\n",
    "    is at zero.\n",
    "    \"\"\"\n",
    "    x_min = tf.reduce_min(x, axis=0)\n",
    "    c = tf.zeros_like(x_min)\n",
    "    norm = tf.where(x_min < 0, -x_min, c)\n",
    "    return x + norm\n",
    "\n",
    "\n",
    "def logsumexp_masked(risk_scores: tf.Tensor,\n",
    "                     mask: tf.Tensor,\n",
    "                     axis: int = 0,\n",
    "                     keepdims: Optional[bool] = None) -> tf.Tensor:\n",
    "    \"\"\"Compute logsumexp across `axis` for entries where `mask` is true.\"\"\"\n",
    "    risk_scores.shape.assert_same_rank(mask.shape)\n",
    "\n",
    "    with tf.name_scope(\"logsumexp_masked\"):\n",
    "        mask_f = tf.cast(mask, risk_scores.dtype)\n",
    "        risk_scores_masked = tf.math.multiply(risk_scores, mask_f)\n",
    "        # for numerical stability, substract the maximum value\n",
    "        # before taking the exponential\n",
    "        amax = tf.reduce_max(risk_scores_masked, axis=axis, keepdims=True)\n",
    "        risk_scores_shift = risk_scores_masked - amax\n",
    "\n",
    "        exp_masked = tf.math.multiply(tf.exp(risk_scores_shift), mask_f)\n",
    "        exp_sum = tf.reduce_sum(exp_masked, axis=axis, keepdims=True)\n",
    "        output = amax + tf.math.log(exp_sum)\n",
    "        if not keepdims:\n",
    "            output = tf.squeeze(output, axis=axis)\n",
    "    return output\n",
    "\n",
    "\n",
    "class CoxPHLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"Negative partial log-likelihood of Cox's proportional hazards model.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)            \n",
    "\n",
    "    def call(self,\n",
    "             y_true: Sequence[tf.Tensor],\n",
    "             y_pred: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"Compute loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : list|tuple of tf.Tensor\n",
    "            The first element holds a binary vector where 1\n",
    "            indicates an event 0 censoring.\n",
    "            The second element holds the riskset, a\n",
    "            boolean matrix where the `i`-th row denotes the\n",
    "            risk set of the `i`-th instance, i.e. the indices `j`\n",
    "            for which the observer time `y_j >= y_i`.\n",
    "            Both must be rank 2 tensors.\n",
    "        y_pred : tf.Tensor\n",
    "            The predicted outputs. Must be a rank 2 tensor.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        loss : tf.Tensor\n",
    "            Loss for each instance in the batch.\n",
    "        \"\"\"\n",
    "        event, riskset = y_true\n",
    "        predictions = y_pred\n",
    "        print(event.shape)\n",
    "        print(predictions.shape)\n",
    "        pred_shape = predictions.shape\n",
    "        if pred_shape.ndims != 2:\n",
    "            raise ValueError(\"Rank mismatch: Rank of predictions (received %s) should \"\n",
    "                             \"be 2.\" % pred_shape.ndims)\n",
    "\n",
    "        if pred_shape[1] is None:\n",
    "            raise ValueError(\"Last dimension of predictions must be known.\")\n",
    "\n",
    "        if pred_shape[1] != 1:\n",
    "            raise ValueError(\"Dimension mismatch: Last dimension of predictions \"\n",
    "                             \"(received %s) must be 1.\" % pred_shape[1])\n",
    "\n",
    "        if event.shape.ndims != pred_shape.ndims:\n",
    "            raise ValueError(\"Rank mismatch: Rank of predictions (received %s) should \"\n",
    "                             \"equal rank of event (received %s)\" % (\n",
    "                pred_shape.ndims, event.shape.ndims))\n",
    "\n",
    "        if riskset.shape.ndims != 2:\n",
    "            raise ValueError(\"Rank mismatch: Rank of riskset (received %s) should \"\n",
    "                             \"be 2.\" % riskset.shape.ndims)\n",
    "\n",
    "        event = tf.cast(event, predictions.dtype)\n",
    "        predictions = safe_normalize(predictions)\n",
    "        with tf.name_scope(\"assertions\"):\n",
    "            assertions = (\n",
    "                tf.debugging.assert_less_equal(event, 1.),\n",
    "                tf.debugging.assert_greater_equal(event, 0.),\n",
    "                tf.debugging.assert_type(riskset, tf.bool)\n",
    "            )\n",
    "\n",
    "        # move batch dimension to the end so predictions get broadcast\n",
    "        # row-wise when multiplying by riskset\n",
    "        pred_t = tf.transpose(predictions)\n",
    "        # compute log of sum over risk set for each row\n",
    "        rr = logsumexp_masked(pred_t, riskset, axis=1, keepdims=True)\n",
    "        assert rr.shape.as_list() == predictions.shape.as_list()\n",
    "\n",
    "        losses = tf.math.multiply(event, rr - predictions)\n",
    "\n",
    "        return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cec095ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CindexMetric:\n",
    "    \"\"\"Computes concordance index across one epoch.\"\"\"\n",
    "\n",
    "    def reset_states(self) -> None:\n",
    "        \"\"\"Clear the buffer of collected values.\"\"\"\n",
    "        self._data = {\n",
    "            \"label_time\": [],\n",
    "            \"label_event\": [],\n",
    "            \"prediction\": []\n",
    "        }\n",
    "\n",
    "    def update_state(self, y_true: Dict[str, tf.Tensor], y_pred: tf.Tensor) -> None:\n",
    "        \"\"\"Collect observed time, event indicator and predictions for a batch.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_true : dict\n",
    "            Must have two items:\n",
    "            `label_time`, a tensor containing observed time for one batch,\n",
    "            and `label_event`, a tensor containing event indicator for one batch.\n",
    "        y_pred : tf.Tensor\n",
    "            Tensor containing predicted risk score for one batch.\n",
    "        \"\"\"\n",
    "        self._data[\"label_time\"].append(y_true[\"label_time\"].numpy())\n",
    "        self._data[\"label_event\"].append(y_true[\"label_event\"].numpy())\n",
    "        self._data[\"prediction\"].append(tf.squeeze(y_pred).numpy())\n",
    "\n",
    "    def result(self) -> Dict[str, float]:\n",
    "        \"\"\"Computes the concordance index across collected values.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        metrics : dict\n",
    "            Computed metrics.\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        for k, v in self._data.items():\n",
    "            data[k] = np.concatenate(v)\n",
    "\n",
    "        results = concordance_index_censored(\n",
    "            data[\"label_event\"] == 1,\n",
    "            data[\"label_time\"],\n",
    "            data[\"prediction\"])\n",
    "\n",
    "        result_data = {}\n",
    "        names = (\"cindex\", \"concordant\", \"discordant\", \"tied_risk\")\n",
    "        for k, v in zip(names, results):\n",
    "            result_data[k] = v\n",
    "        return result_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c5f9433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v2.summary as summary\n",
    "from tensorflow.python.ops import summary_ops_v2\n",
    "\n",
    "\n",
    "class TrainAndEvaluateModel:\n",
    "\n",
    "    def __init__(self, model, model_dir, train_dataset, eval_dataset,\n",
    "                 learning_rate, num_epochs):\n",
    "        self.num_epochs = num_epochs\n",
    "        self.model_dir = model_dir\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        self.train_ds = train_dataset\n",
    "        self.val_ds = eval_dataset\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        self.loss_fn = CoxPHLoss()\n",
    "\n",
    "        self.train_loss_metric = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "        self.val_loss_metric = tf.keras.metrics.Mean(name=\"val_loss\")\n",
    "        self.val_cindex_metric = CindexMetric()\n",
    "\n",
    "    @tf.function\n",
    "    def train_one_step(self, x, y_event, y_riskset):\n",
    "        y_event = tf.expand_dims(y_event, axis=1)\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = self.model(x, training=True)\n",
    "\n",
    "            train_loss = self.loss_fn(y_true=[y_event, y_riskset], y_pred=logits)\n",
    "\n",
    "        with tf.name_scope(\"gradients\"):\n",
    "            grads = tape.gradient(train_loss, self.model.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n",
    "        return train_loss, logits\n",
    "\n",
    "    def train_and_evaluate(self):\n",
    "        ckpt = tf.train.Checkpoint(\n",
    "            step=tf.Variable(0, dtype=tf.int64),\n",
    "            optimizer=self.optimizer,\n",
    "            model=self.model)\n",
    "        ckpt_manager = tf.train.CheckpointManager(\n",
    "            ckpt, str(self.model_dir), max_to_keep=2)\n",
    "\n",
    "        if ckpt_manager.latest_checkpoint:\n",
    "            ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Latest checkpoint restored from {ckpt_manager.latest_checkpoint}.\")\n",
    "\n",
    "        train_summary_writer = summary.create_file_writer(\n",
    "            str(self.model_dir / \"train\"))\n",
    "        val_summary_writer = summary.create_file_writer(\n",
    "            str(self.model_dir / \"valid\"))\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            with train_summary_writer.as_default():\n",
    "                self.train_one_epoch(ckpt.step)\n",
    "\n",
    "            # Run a validation loop at the end of each epoch.\n",
    "            with val_summary_writer.as_default():\n",
    "                self.evaluate(ckpt.step)\n",
    "\n",
    "        save_path = ckpt_manager.save()\n",
    "        print(f\"Saved checkpoint for step {ckpt.step.numpy()}: {save_path}\")\n",
    "    def train_one_epoch(self, step_counter):\n",
    "        for x, y in self.train_ds:\n",
    "            x=tf.transpose(x,[0,1,4,3,2])\n",
    "            train_loss, logits = self.train_one_step(\n",
    "                x, y[\"label_event\"], y[\"label_riskset\"])\n",
    "\n",
    "            step = int(step_counter)\n",
    "            if step == 0:\n",
    "                # see https://stackoverflow.com/questions/58843269/display-graph-using-tensorflow-v2-0-in-tensorboard\n",
    "                func = self.train_one_step.get_concrete_function(\n",
    "                    x, y[\"label_event\"], y[\"label_riskset\"])\n",
    "                summary_ops_v2.graph(func.graph)\n",
    "\n",
    "            # Update training metric.\n",
    "            self.train_loss_metric.update_state(train_loss)\n",
    "\n",
    "            # Log every 200 batches.\n",
    "            if step % 200 == 0:\n",
    "                # Display metrics\n",
    "                mean_loss = self.train_loss_metric.result()\n",
    "                print(f\"step {step}: mean loss = {mean_loss:.4f}\")\n",
    "                # save summaries\n",
    "                summary.scalar(\"loss\", mean_loss, step=step_counter)\n",
    "                # Reset training metrics\n",
    "                self.train_loss_metric.reset_states()\n",
    "\n",
    "            step_counter.assign_add(1)\n",
    "\n",
    "    @tf.function\n",
    "    def evaluate_one_step(self, x, y_event, y_riskset):\n",
    "        x=tf.transpose(x,[0,1,4,3,2])\n",
    "        y_event = tf.expand_dims(y_event, axis=1)\n",
    "        val_logits = self.model(x, training=False)\n",
    "        val_loss = self.loss_fn(y_true=[y_event, y_riskset], y_pred=val_logits)\n",
    "        return val_loss, val_logits\n",
    "\n",
    "    def evaluate(self, step_counter):\n",
    "        self.val_cindex_metric.reset_states()\n",
    "        \n",
    "        for x_val, y_val in self.val_ds:\n",
    "            #x_val=tf.transpose(x_val,[0,1,4,3,2])\n",
    "            val_loss, val_logits = self.evaluate_one_step(\n",
    "                x_val, y_val[\"label_event\"], y_val[\"label_riskset\"])\n",
    "\n",
    "            # Update val metrics\n",
    "            self.val_loss_metric.update_state(val_loss)\n",
    "            self.val_cindex_metric.update_state(y_val, val_logits)\n",
    "\n",
    "        val_loss = self.val_loss_metric.result()\n",
    "        summary.scalar(\"loss\",\n",
    "                       val_loss,\n",
    "                       step=step_counter)\n",
    "        self.val_loss_metric.reset_states()\n",
    "        \n",
    "        val_cindex = self.val_cindex_metric.result()\n",
    "        for key, value in val_cindex.items():\n",
    "            summary.scalar(key, value, step=step_counter)\n",
    "\n",
    "        print(f\"Validation: loss = {val_loss:.4f}, cindex = {val_cindex['cindex']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e55911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43b958bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.168421</td>\n",
       "      <td>32.839078</td>\n",
       "      <td>31.162122</td>\n",
       "      <td>13.442088</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.638114</td>\n",
       "      <td>26.705598</td>\n",
       "      <td>31.168813</td>\n",
       "      <td>27.159596</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.799805</td>\n",
       "      <td>30.558642</td>\n",
       "      <td>21.663643</td>\n",
       "      <td>28.643374</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.791226</td>\n",
       "      <td>32.875502</td>\n",
       "      <td>28.867748</td>\n",
       "      <td>33.600624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.116384</td>\n",
       "      <td>34.917555</td>\n",
       "      <td>21.993838</td>\n",
       "      <td>26.748109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.601407</td>\n",
       "      <td>31.745358</td>\n",
       "      <td>25.368741</td>\n",
       "      <td>21.871702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.788078</td>\n",
       "      <td>28.580090</td>\n",
       "      <td>24.719658</td>\n",
       "      <td>22.192359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.840679</td>\n",
       "      <td>28.824603</td>\n",
       "      <td>34.525343</td>\n",
       "      <td>25.742900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.769208</td>\n",
       "      <td>20.826594</td>\n",
       "      <td>31.157497</td>\n",
       "      <td>16.333807</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.970887</td>\n",
       "      <td>25.538452</td>\n",
       "      <td>24.989061</td>\n",
       "      <td>34.535018</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14125</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.353400</td>\n",
       "      <td>28.803313</td>\n",
       "      <td>21.845824</td>\n",
       "      <td>32.030173</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.827361</td>\n",
       "      <td>32.637903</td>\n",
       "      <td>22.007035</td>\n",
       "      <td>30.355449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14126</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.872961</td>\n",
       "      <td>32.763951</td>\n",
       "      <td>33.876846</td>\n",
       "      <td>33.286718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.696549</td>\n",
       "      <td>36.546956</td>\n",
       "      <td>25.913325</td>\n",
       "      <td>33.426996</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14127</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.735955</td>\n",
       "      <td>21.600607</td>\n",
       "      <td>22.125052</td>\n",
       "      <td>31.711105</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.178115</td>\n",
       "      <td>35.294532</td>\n",
       "      <td>31.775084</td>\n",
       "      <td>24.886186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14128</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.316853</td>\n",
       "      <td>26.117956</td>\n",
       "      <td>27.355340</td>\n",
       "      <td>35.736741</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.278579</td>\n",
       "      <td>19.359326</td>\n",
       "      <td>16.594623</td>\n",
       "      <td>21.571014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14129</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.973354</td>\n",
       "      <td>2.848393</td>\n",
       "      <td>-2.145590</td>\n",
       "      <td>-7.182469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-6.181750</td>\n",
       "      <td>-5.219940</td>\n",
       "      <td>3.485965</td>\n",
       "      <td>6.529571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14130 rows × 72 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2          3          4          5          6    7    8   \\\n",
       "0      0.0  0.0  0.0  20.168421  32.839078  31.162122  13.442088  0.0  0.0   \n",
       "1      0.0  0.0  0.0  19.799805  30.558642  21.663643  28.643374  0.0  0.0   \n",
       "2      0.0  0.0  0.0  26.116384  34.917555  21.993838  26.748109  0.0  0.0   \n",
       "3      0.0  0.0  0.0  25.788078  28.580090  24.719658  22.192359  0.0  0.0   \n",
       "4      0.0  0.0  0.0  17.769208  20.826594  31.157497  16.333807  0.0  0.0   \n",
       "...    ...  ...  ...        ...        ...        ...        ...  ...  ...   \n",
       "14125  0.0  0.0  0.0  19.353400  28.803313  21.845824  32.030173  0.0  0.0   \n",
       "14126  0.0  0.0  0.0  15.872961  32.763951  33.876846  33.286718  0.0  0.0   \n",
       "14127  0.0  0.0  0.0  32.735955  21.600607  22.125052  31.711105  0.0  0.0   \n",
       "14128  0.0  0.0  0.0  32.316853  26.117956  27.355340  35.736741  0.0  0.0   \n",
       "14129  0.0  0.0  0.0  -8.973354   2.848393  -2.145590  -7.182469  0.0  0.0   \n",
       "\n",
       "        9   ...   62   63   64   65         66         67         68  \\\n",
       "0      0.0  ...  0.0  0.0  0.0  0.0  35.638114  26.705598  31.168813   \n",
       "1      0.0  ...  0.0  0.0  0.0  0.0  28.791226  32.875502  28.867748   \n",
       "2      0.0  ...  0.0  0.0  0.0  0.0  32.601407  31.745358  25.368741   \n",
       "3      0.0  ...  0.0  0.0  0.0  0.0  20.840679  28.824603  34.525343   \n",
       "4      0.0  ...  0.0  0.0  0.0  0.0  24.970887  25.538452  24.989061   \n",
       "...    ...  ...  ...  ...  ...  ...        ...        ...        ...   \n",
       "14125  0.0  ...  0.0  0.0  0.0  0.0  21.827361  32.637903  22.007035   \n",
       "14126  0.0  ...  0.0  0.0  0.0  0.0  23.696549  36.546956  25.913325   \n",
       "14127  0.0  ...  0.0  0.0  0.0  0.0  16.178115  35.294532  31.775084   \n",
       "14128  0.0  ...  0.0  0.0  0.0  0.0  17.278579  19.359326  16.594623   \n",
       "14129  0.0  ...  0.0  0.0  0.0  0.0  -6.181750  -5.219940   3.485965   \n",
       "\n",
       "              69   70   71  \n",
       "0      27.159596  0.0  0.0  \n",
       "1      33.600624  0.0  0.0  \n",
       "2      21.871702  0.0  0.0  \n",
       "3      25.742900  0.0  0.0  \n",
       "4      34.535018  0.0  0.0  \n",
       "...          ...  ...  ...  \n",
       "14125  30.355449  0.0  0.0  \n",
       "14126  33.426996  0.0  0.0  \n",
       "14127  24.886186  0.0  0.0  \n",
       "14128  21.571014  0.0  0.0  \n",
       "14129   6.529571  0.0  0.0  \n",
       "\n",
       "[14130 rows x 72 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "vor_images1=pd.read_csv(\"data/temporal/data_72_arti.csv\",header=None)\n",
    "#vor_images1=vor_images1\n",
    "vor_images1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b8bddd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14130, 72)\n",
      "torch.Size([14130, 1, 8, 9])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "vor_images2=vor_images1.values.reshape(14130, 1,8,9)\n",
    "print(vor_images1.shape)\n",
    "#print(vor_images1)\n",
    "vor_images2=torch.from_numpy(vor_images2)\n",
    "vor_images2=vor_images2.to(torch.float32)\n",
    "print(vor_images2.shape)\n",
    "print(vor_images2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefc4fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14130, 56)\n"
     ]
    }
   ],
   "source": [
    "data_path='data/temporal'\n",
    "#X = np.load(path.join(data_path,'X.csv'))\n",
    "X=pd.read_csv(\"data/temporal/X5_all_arti.csv\")\n",
    "\n",
    "X=X.iloc[:,1:]\n",
    "print(X.shape)\n",
    "#y = np.load(path.join(data_path,'y.npy'))\n",
    "y=pd.read_csv(\"data/temporal/Y5_e_arti.csv\")\n",
    "y=y.iloc[:,1:]\n",
    "\n",
    "\n",
    "#X=X.iloc[:,2:]\n",
    "#X.iloc[:,2:54]=normalize(X.iloc[:,2:54])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b56888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `Sequential` from `keras.models`\n",
    "from keras.layers import MaxPooling3D,MaxPooling2D, TimeDistributed\n",
    "from keras.layers import Conv3D,Flatten,Conv2D\n",
    "from keras.models import Sequential\n",
    "# Import `Dense` from `keras.layers`\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "import keras\n",
    "#main_input = Input(shape=(data_new.shape[1], data_new.shape[2]), name='main_input')\n",
    "model =tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.TimeDistributed(Conv2D(8, kernel_size=3, strides=1,padding=\"same\" ,activation='relu'), input_shape=(5,61,61,1)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2),strides=(2, 2),padding=\"valid\")))\n",
    "model.add(tf.keras.layers.TimeDistributed(Conv2D(8, kernel_size=3, strides=1,padding=\"same\" ,activation='relu'), input_shape=(5,30,30,8)))\n",
    "model.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2),strides=(2, 2),padding=\"valid\")))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "model.add(TimeDistributed(Dense(52, activation='relu')))\n",
    "model.add(LSTM(64, input_shape=(5,52), return_sequences=True))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(16, return_sequences=True))  \n",
    "model.add(LSTM(8))\n",
    "    #tf.keras.layers.Dense(1, activation='linear')\n",
    "    #tf.keras.layers.Dense(1, activation='tanh')\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "#model.add(LSTM(10, dropout = 0.3, recurrent_dropout = 0.3, name=\"LSTM_Layer\"))\n",
    "#model.add(LSTM(10, dropout = 0.3, recurrent_dropout = 0.3, name=\"LSTM_Layer\"))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "            from sklearn.model_selection import  StratifiedKFold \n",
    "            rskf =  StratifiedKFold (n_splits=10, shuffle=True,random_state=412)\n",
    "            index = 1\n",
    "            features_per_folds = []\n",
    "            for train_index, test_index in rskf.split(vor_images2,y.iloc[:,0]):\n",
    "               # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "                #print(\"iter\", index)\n",
    "                index += 1\n",
    "                #X_train, X_test = X.iloc[train_index,2:54], X.iloc[test_index,2:54]\n",
    "                X_train, X_test =vor_images2[train_index,:], vor_images2[test_index,:]\n",
    "                y_train, y_test = X.iloc[train_index,54:56], X.iloc[test_index,54:56]\n",
    "                #print(X_train.shape)\n",
    "                #print(X_test.shape)\n",
    "                t=5\n",
    "                dim1=(int(X_train.shape[0]/t))\n",
    "                dim2=(int(X_test.shape[0]/t))\n",
    "                data_new=np.zeros((dim1,5,1,8,9))\n",
    "                data_new_test=np.zeros((dim2,5,1,8,9))\n",
    "                y_new=np.zeros((dim1,2))\n",
    "                y_new_test=np.zeros((dim2,2))\n",
    "#X_train0=X_train.values.tolist()\n",
    "                #X_train0=X_train.values\n",
    "                #print(X_train0.shape())\n",
    "                #X_train_lists = [X_train0[i:i + t] for i in range(0, len(X_train0), t)]\n",
    "                X_train_lists = [X_train[i:i + t] for i in range(0, X_train.shape[0], t)]\n",
    "                X_train_lists_array=np.asarray(X_train_lists)\n",
    "\n",
    "                y_train0=y_train.values\n",
    "                y_train_lists = [y_train0[i:i + t] for i in range(0, len(y_train0), t)]\n",
    "                y_train_lists_array=np.asarray(y_train_lists)\n",
    "\n",
    "\n",
    "\n",
    "                #X_test0=X_test.values\n",
    "                X_test_lists = [X_test[i:i + t] for i in range(0, X_test.shape[0], t)]\n",
    "                X_test_lists_array=np.asarray(X_test_lists)\n",
    "\n",
    "                y_test0=y_test.values\n",
    "                y_test_lists = [y_test0[i:i + t] for i in range(0, len(y_test0), t)]\n",
    "                y_test_lists_array=np.asarray(y_test_lists)\n",
    "                for i in range(int(X_train.shape[0]/t)):\n",
    "                    #print(X_train_lists_array[i].shape)\n",
    "                    data_new[i]=X_train_lists_array[i]\n",
    "   \n",
    "                    y_new[i]=y_train_lists_array[i][1]\n",
    "    #return(data_new)\n",
    "#data_new.shape\n",
    "                for i in range(int(X_test.shape[0]/t)):\n",
    "                   data_new_test[i]=X_test_lists_array[i]\n",
    "                   y_new_test[i]=y_test_lists_array[i][1]\n",
    "    #return(data_new)\n",
    "                trainer.train_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
